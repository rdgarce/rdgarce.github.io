<!DOCTYPE html>
<html lang="en">
    
<head>
    <link rel="stylesheet" href="../style.css">
    <meta charset="UTF-8">
    <title>Raffaele del Gaudio | Are you sure you can implement a queue?</title>
</head>
<body>
<h1>Are you sure you can implement a queue?</h1>

<p><i>Published on 10 September 2025</i></p>

<p>
I often use queues for work and in personal projects, and I think you do too; for instance you may need to save bytes received from a socket for
later processing, or to <a href="read_faster_than_stdio.html">read from a file faster than stdio</a>. Anytime you need two entities
to communicate asynchronously in a FIFO pattern, you use a queue.
</p>
<p>
The most common implementation of a queue is the <a href="https://en.wikipedia.org/wiki/Circular_buffer" target="_blank">circular buffer</a>,
which uses a fixed-size buffer and two cursors over it, usually referred to as <i>head</i> and <i>tail</i>. They index the location of the next read
and the next write. If you were to google for a queue implementation or ask ChatGPT to implement one, you would probably get something like this one:
</p>

<div class="code-block">
typedef struct
{
    uint8_t *data;
    size_t head;
    size_t tail;
    size_t nelem;
    size_t size;
} ByteQueue;

void queue_init(ByteQueue *q, uint8_t *buffer, size_t size)
{
    q->data = buffer;
    q->size = size;
    q->head = 0;
    q->tail = 0;
    q->nelem = 0;
}

bool queue_push(ByteQueue *q, uint8_t byte)
{
    bool ok = false;
    if (q->nelem < q->size)
    {
        q->data[q->tail] = byte;
        q->tail = (q->tail + 1) % q->size;
        q->nelem++;
        ok = true;
    }
    return ok;
}

bool queue_pop(ByteQueue *q, uint8_t *byte)
{
    bool ok = false;
    if (q->nelem > 0)
    {
        *byte = q->data[q->head];
        q->head = (q->head + 1) % q->size;
        q->nelem--;
        ok = true;
    }
    return ok;
}
</div>

<p>
For the sake of future references, let's name the previous implementation "Basic Byte Queue (BBQ)".
</p>
<p>
I used this style of queue for quite a bit of time, but always felt unease as I had the feeling it could be improved quite a bit.
Only recently I stopped and thought about how it could be improved. So, this article shows my process of improvement and how I got
to an implementation that is 1000x faster, more elegant, and that provides more features.To put things in perspective, 1000x is
the difference between processing 1 GB of data in 30 ms instead of 30 s!
</p>

<p>
All the code shown in this article, along with the final implementation and the tests for correctness and performance, is available
in the <a href="https://github.com/rdgarce/bq" target="_blank">github repo</a>.
</p>

<h2>Changing the API</h2>

<p>
The starting point is the API: why call the push/pop functions on each byte? Wouldn't it be better if we managed bytes in batches?
This would greatly reduce the cost associated to function calls, such as preparing the
<a href="https://en.wikipedia.org/wiki/Call_stack#STACK-FRAME" target="_blank">stack frame</a> and jumping to the function.
Apart from this overhead, you also have a number of operations that you could perform once for all the bytes to push/pop
instead of doing them for each single byte.
</p>
<p>
This is how the queue_push and queue_pop operations look like with this modification:
</p>

<div class="code-block">
size_t queue_push_vector(ByteQueue *q, const uint8_t *bytes, size_t count)
{
    size_t tailcopy_size;
    size_t total = 0;

    if (q->head < q->tail || q->nelem == 0)
    {
        tailcopy_size = MIN(q->size - q->tail, count);
        memcpy(q->data + q->tail, bytes, tailcopy_size);
        q->tail = (q->tail + tailcopy_size) % q->size;
        total += tailcopy_size;
    }

    size_t headcopy_size = MIN(q->head - q->tail, count - tailcopy_size);
    if (headcopy_size > 0)
    {
        memcpy(q->data + q->tail, bytes + tailcopy_size, headcopy_size);
        q->tail += headcopy_size;
        total += headcopy_size;
    }
    
    q->nelem += total;    

    return total;
}

size_t queue_pop_vector(ByteQueue *q, uint8_t *bytes, size_t count)
{
    if (q->nelem == 0) return 0;

    if (q->head < q->tail)
    {
        size_t to_pop = MIN(q->tail - q->head, count);
        memcpy(bytes, q->data + q->head, to_pop);
        q->head = (q->head + to_pop) % q->size;
        q->nelem -= to_pop;
        return to_pop;
    }
    else
    {
        // q->head >= q->tail
        size_t frst_pop = MIN(q->size - q->head, count);
        memcpy(bytes, q->data + q->head, frst_pop);
        q->head = (q->head + frst_pop) % q->size;
        q->nelem -= frst_pop;
        
        size_t scnd_pop = MIN(q->tail - q->head, count - frst_pop);
        if (scnd_pop > 0)
        {
            memcpy(bytes + frst_pop, q->data + q->head, scnd_pop);
            q->head = (q->head + scnd_pop) % q->size;
            q->nelem -= scnd_pop;
        }

        return frst_pop + scnd_pop;
    }
}
</div>

<p>
Let's call this implementation the "Vectorial Byte Queue (VBQ)".
</p>

<p>
With this API, the push operation expects a pointer to an array of bytes and its length and it tries to copy all of them in the queue.
The pop operation does something similar in the opposite direction. 
</p>

<h2>Removing unnecessary copies</h2>

<p>
The second modification comes by noticing that the most expensive operation on computers in terms of latency is accessing memory.
Roughly, accessing RAM requires 10x the time needed to access L2 cache, and accessing the L2 requires 10x the time needed for accessing the L1.
</p>

<p>
The rule of thumb here is: <b>always avoid unnecessary copies!</b>
</p>

<p>
We are actually doing an unnecessary copy in the previous implementation because the push operation could provide a direct pointer to the
internal queue memory and the maximum number of bytes the user can write, and the same applies for the pop operation.
</p>

<p>
This is how the two operations look like after applying this change:
</p>

<div class="code-block">
void *queue_get_push_buf(ByteQueue *q, size_t *len)
{
    if (q->head < q->tail || q->nelem == 0)
        *len = q->size - q->tail;
    else // q->head >= q->tail
        *len = q->head - q->tail;

    return q->data + q->tail;
}

void queue_commit_push(ByteQueue *q, size_t len)
{
    q->tail = (q->tail + len) % q->size;
    q->nelem += len;
}

void *queue_get_pop_buf(ByteQueue *q, size_t *len)
{
    if (q->head < q->tail || q->nelem == 0)
        *len = q->tail - q->head;
    else // q->head >= q->tail
        *len = q->size - q->head;

    return q->data + q->head;
}

void queue_commit_pop(ByteQueue *q, size_t len)
{
    q->head = (q->head + len) % q->size;
    q->nelem -= len;
}
</div>

<p>
Let's call this implementation the "Acked Byte Queue (ABQ)".
</p>

<p>
With this, the user needs to commit the operations to inform the queue on how much bytes he pushed or popped but you
can clearly see that this code allows the program to skip one useless data copy.
</p>

<h2>Optimizing the state representation</h2>

<p>
Another step forward is reducing the internal state to just two of the three variables: <code class="inline-code">head</code>,
<code class="inline-code">tail</code>, and <code class="inline-code">nelem</code>. The reason we have them in the first place is that when
<code class="inline-code">head == tail</code>, it’s impossible to distinguish whether the queue is empty or full once the producer
has filled it. Some implementations remove <code class="inline-code">nelem</code> and use a special value (e.g. -1) for
<code class="inline-code">head</code> and <code class="inline-code">tail</code> to indicate emptiness; others remove
<code class="inline-code">tail</code> and only rely on <code class="inline-code">head</code> and <code class="inline-code">nelem</code>.
But there’s also a third, more elegant solution: using only <code class="inline-code">head</code> and <code class="inline-code">tail</code>,
without any special marker, while still being able to clearly distinguish between an empty and a full queue.
</p>

<p>
By restricting the queue length to a power of two and keeping <code class="inline-code">head</code> and <code class="inline-code">tail</code>
as ever-increasing counters (without applying modulo immediately), it becomes straightforward to distinguish between an empty and a full queue.
In this scheme, <code class="inline-code">head == tail</code> always indicates that the queue is empty, while
<code class="inline-code">(tail - head) == capacity</code> always indicates that it is full. Even when <code class="inline-code">tail</code>
overflows and wraps around <code class="inline-code">SIZE_MAX</code>, the implicit modulo arithmetic
<code class="inline-code">(mod SIZE_MAX+1)</code> ensures correctness. The actual index into the buffer can then be computed by applying the
modulo operation only when needed for access.
</p>

<p>
<div class="note">
<b>NOTE</b>: When storing head and tail without the modulo, the queue length MUST be a power of 2, otherwise the implicit modulo will lead
to incorrect states. As an example (provided by skeeto while reviewing this article):
Suppose the queue has <code class="inline-code">capacity == 3</code>, and currently <code class="inline-code">head == SIZE_MAX</code>,
about to tick over to zero. To pop the next element you access <code class="inline-code">data[head % 3] == data[0]</code>, because
<code class="inline-code">SIZE_MAX % 3 == 0</code>. After the pop, you update:
<code class="inline-code">head = head + 1 == 0</code>, because <code class="inline-code">SIZE_MAX + 1 == 0</code>.
We've incremented head but the effective value is still zero!
</div>
</p>

<h2>Making it MT-safe and lock-free</h2>

<p>
By using <code class="inline-code">head</code> and <code class="inline-code">tail</code>, we gain the additional benefit that the queue can be made MT-safe with only minimal
modification. If we only allow one producer thread and one consumer thread, we can even avoid locks, making the implementation entirely lock-free.
</p>

<p>
The only needed thing is that <code class="inline-code">head</code> and <code class="inline-code">tail</code> are accessed with
<a href="https://research.swtch.com/plmm#acqrel" target="_blank">acquire/release consistency</a> to be sure that the result of buffer memory
is accessed in the correct order with respect with state updates.
If <code class="inline-code">head</code> and <code class="inline-code">nelem</code> were used to represent the state,
both would need to be updated atomically by the consumer after a pop; otherwise, the state could become inconsistent.
This would either require a lock or storing both variables in a single word that can be updated atomically.
Compared to the previous approach, however, this feels like an unnecessary complication.
</p>

<p>
This is how the operations look like after applying the two previous observations:
</p>

<div class="code-block">
void *queue_get_push_buf(ByteQueue *q, size_t *len)
{
    size_t head = __atomic_load_n(&q->head, __ATOMIC_ACQUIRE);
    size_t head_n_wrap = head / q->size;
    size_t tail_n_wrap = q->tail / q->size;

    if (head_n_wrap == tail_n_wrap)
        // head and tail are in the same block of
        // q->size bytes
        *len = q->size - (q->tail % q->size);
    else
        *len = q->size - (q->tail - head);

    return q->data + (q->tail % q->size);
}

void queue_commit_push(ByteQueue *q, size_t len)
{
    __atomic_store_n(&q->tail, q->tail + len, __ATOMIC_RELEASE);
}

void *queue_get_pop_buf(ByteQueue *q, size_t *len)
{
    size_t tail = __atomic_load_n(&q->tail, __ATOMIC_ACQUIRE);
    size_t head_n_wrap = q->head / q->size;
    size_t tail_n_wrap = tail / q->size;

    if (head_n_wrap == tail_n_wrap)
        // head and tail are in the same block of
        // q->size bytes
        *len = tail - q->head;
    else // q->head >= q->tail
        *len = tail - q->head - (tail % q->size);

    return q->data + (q->head % q->size);
}

void queue_commit_pop(ByteQueue *q, size_t len)
{
    __atomic_store_n(&q->head, q->head + len, __ATOMIC_RELEASE);
}
</div>

<p>
Let's call this implementation the "Lock-Free Queue (LFQ)".
</p>

<h2>Removing divisions and branches</h2>

<p>
There are other two improvements that we can make:
</p>

<ol>
    <li>Eliminate divisions and modulo operations with bitwise shifts and ANDs, which comes for free by having imposed a power of 2 as length.</li>
    <li>Remove the if statements to make the implementation completely branchless.</li>
</ol>

<p>
Let's have a look at a piece of the final implementation available in the <a href="https://github.com/rdgarce/bq" target="_blank">github repo</a>:
</p>

<div class="code-block">
static void *bq_popbuf(bq *q, size_t *len)
{
    size_t tail = __atomic_load_n(&q->tail, __ATOMIC_ACQUIRE);
    // The cond variable is 0 iff tail is in the same block of
    // (q->mask + 1) bytes, otherwise is:
    // -- 1, if tail is in the next block and has not wrapped around SIZE_MAX,
    // -- A big odd negative number, when tail has wrapped around SIZE_MAX.
    // With the final bitwise AND, we reduce the possible results to (0, 1).
    // We use this variable to subtract from the final number conditionally.
    size_t cond = ((tail >> q->cap_lg2) - (q->head >> q->cap_lg2)) & 0x1;
    *len = tail - q->head - (tail & q->mask) * cond;

    return q->data + (q->head & q->mask);
}

static void bq_pop(bq *q, size_t count)
{
    __atomic_store_n(&q->head, q->head + count, __ATOMIC_RELEASE);
}

static void *bq_pushbuf(bq *q, size_t *len)
{
    size_t head = __atomic_load_n(&q->head, __ATOMIC_ACQUIRE);
    // The cond variable is 0 iff tail is in the same block of
    // (q->mask + 1) bytes, otherwise is:
    // -- 1, if tail is in the next block and has not wrapped around SIZE_MAX,
    // -- A big odd negative number, when tail has wrapped around SIZE_MAX.
    // With the final bitwise AND, we reduce the possible results to (0, 1).
    // We use this variable to subtract from the final number conditionally.
    size_t cond = ((q->tail >> q->cap_lg2) - (head >> q->cap_lg2)) & 0x1;
    *len = q->mask + 1 - (q->tail - head) - (head & q->mask) * (1 - cond);

    return q->data + (q->tail & q->mask);
}

static void bq_push(bq *q, size_t count)
{
    __atomic_store_n(&q->tail, q->tail + count, __ATOMIC_RELEASE);
}
</div>

<p>
Let's call this implementation the "Byte Queue (BQ)".
</p>

<p>
The <code class="inline-code">cond</code> variable lets us eliminate explicit if/else branches by turning the conditional adjustment of
<code class="inline-code">*len</code> into a simple multiplication. In other words, instead of branching, we encode the
“only when <code class="inline-code">cond == 1</code>” logic directly into the arithmetic expression.
</p>

<h2>Perfomance comparison</h2>

<p>
Now let's have some fun and compare the speed of these different implementations.
</p>

<p>
<div class="note">
<b>Note:</b> Since they are tested in a two-threads scenario,
we need to add a mutex to protect the BBQ, VBQ and ABQ operations so that they become MT-safe.
</div>
</p>

<p>
The following plot compares the time required to transfer 1 GB of data through the queues. The queue size is fixed at 1 MB, and the plot shows
the clock cycles per byte as a function of the maximum number of bytes allowed per operation.
</p>

<img src="media/queue_comparation.svg" alt="Processing 1 GB of data on queues of 1MB of length" style="width: 100%; height: auto;">

<p>
We can see that the major source of overhead reduction is from BBQ to VBQ because of the elimination of all those function calls.
This optimization alone generates a minimum reduction of 8x and a maximum of roughly 1600x in the number of cycles per byte and the
performance gain you get from this optimization grows with how much data you can process per operation.
</p>

<p>
The second most impacting optimization is the one from VBQ to ABQ because of the reduction of the unnecessary copy. The performance
gain you get from this optimization grows with how much data you can process per operation; indeed you get 20%/30% reduction with relatively
few bytes per operation but if you can move more than 64 KB per operation you can get more than 2x reduction in the number of cycles per byte.
</p>

<p>
The performance gain you get from the last two optimizations (LFQ and BQ) decreases with how much data you can process per operation because,
when moving a lot of bytes at once, the clock cycles needed to lock and unlock the mutex and do divisions and modulo operations are very few
compared to those needed to actually move the data. In other words, if you are able to move a lot of bytes per operation, the number of calls
to the queue functions will decrease, so optimizing them will have less impact on performance. That said, when moving 64 Bytes or 512 Bytes per
operation you get roughly 18% reduction in the number of cycles per byte: not bad!
</p>

<h2>Any feedback?</h2>

<p>
If you have any questions or feedback regarding this article, you can find my contact information on the <a href="../index.html">homepage</a>.
</p>

<h2>Acknowledgements</h2>

<a href="https://coz.is/" target="_blank">cozis</a>, <a href="https://nullprogram.com/" target="_blank">skeeto</a>
and <a href="https://purplesyringa.moe/" target="_blank">purplesyringa</a> for reviewing the article and providing insightful
<a href="https://www.reddit.com/r/C_Programming/comments/1ne4bc0/are_you_sure_you_can_implement_a_queue" target="_blank">comments</a>.

</body>
</html>